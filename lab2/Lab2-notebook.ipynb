{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Laboratoire 2: Optimisation sans contraintes\n",
    "Tangi Migot et Paul Raynaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\") #Accède au fichier Project.toml\n",
    "Pkg.instantiate()\n",
    "Pkg.status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ADNLPModels, LinearAlgebra, NLPModels, Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra trouver de la documentation sur `ADNLPModels` et `NLPModels` ici:\n",
    "- [juliasmoothoptimizers.github.io/NLPModels.jl/dev/](https://juliasmoothoptimizers.github.io/NLPModels.jl/dev/)\n",
    "- [juliasmoothoptimizers.github.io/ADNLPModels.jl/dev/](https://juliasmoothoptimizers.github.io/ADNLPModels.jl/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problème test:\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1) # fonction objectif vue en classe\n",
    "g(x) = 6 * [x[1]^2 - x[1] - 2*x[1]*x[2] + x[2]^2 + x[2]; -x[1]^2 + 2*x[1]*x[2] + x[1]] # le gradient de f\n",
    "H(x) = 6 * [2*x[1]-1-2*x[2] -2*x[1]+2*x[2]+1; -2*x[1]+2*x[2]+1 2*x[1]] # la Hessienne de f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Newton avec recherche linéaire - amélioration du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, vous avez le code de deux fonctions qui ont été vues dans le cours, la recherche linéaire qui satisfait Armijo, et une méthode de Newton avec cette recherche linéaire. Le but de ce laboratoire est d'implémenter d'autres méthodes utiles pour résoudre des problèmes de grandes dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amélioration possibles: return also the value of f\n",
    "function armijo(xk, dk, fk, gk, f)\n",
    "  slope = dot(gk, dk) #doit être <0\n",
    "  t = 1.0\n",
    "  while f(xk + t * dk) > fk + 1.0e-4 * t * slope\n",
    "    t /= 1.5\n",
    "  end\n",
    "  return t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test pour vérifier que la fonction armijo fonctionne correctement.\n",
    "using Test #le package Test définit (entre autre) la macro @test qui permet de faire des tests unitaires :-)\n",
    "xk = ones(2)\n",
    "gk = g(xk)\n",
    "dk = - gk\n",
    "fk = f(xk)\n",
    "t  = armijo(xk, dk, fk, gk, f)\n",
    "@test t < 1\n",
    "@test f(xk + t * dk) <= fk + 1.0e-4 * t * dot(gk,dk)\n",
    "\n",
    "xk = [1.5, 0.5]\n",
    "fk = f(xk)\n",
    "gk = g(xk)\n",
    "dk = - gk\n",
    "t  = armijo(xk, dk, fk, gk, f)\n",
    "@test t < 1\n",
    "@test f(xk + t * dk) <= f(xk) + 1.0e-4 * t * dot(g(xk),dk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_armijo(f, g, H, x0; verbose::Bool = true)\n",
    "  xk  = x0\n",
    "  fk  = f(xk)\n",
    "  gk = g(xk)\n",
    "  gnorm = gnorm0 = norm(gk)\n",
    "  k = 0\n",
    "  verbose && @printf \"%2s %9s %9s\\n\" \"k\" \"fk\" \"||∇f(x)||\"\n",
    "  verbose && @printf \"%2d %9.2e %9.1e\\n\" k fk gnorm\n",
    "  while gnorm > 1.0e-6 + 1.0e-6 * gnorm0 && k < 100\n",
    "    Hk = H(xk)\n",
    "    dk = - Hk \\ gk\n",
    "    slope = dot(dk, gk)\n",
    "    λ = 0.0\n",
    "    while slope ≥ -1.0e-4 * norm(dk) * gnorm\n",
    "      λ = max(1.0e-3, 10 * λ)\n",
    "      dk = - ((Hk + λ * I ) \\ gk)\n",
    "      slope = dot(dk, gk)\n",
    "    end\n",
    "    t = armijo(xk, dk, fk, gk, f)\n",
    "    xk += t * dk\n",
    "    fk = f(xk)\n",
    "    gk = g(xk)\n",
    "    gnorm = norm(gk)\n",
    "    k += 1\n",
    "    verbose && @printf \"%2d %9.2e %9.1e %7.1e \\n\" k fk gnorm t\n",
    "  end\n",
    "  return xk\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = newton_armijo(f, g, H, [.5, .5])\n",
    "@test g(sol) ≈ zeros(2) atol = 1.0e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut améliorer le code de la fonction `newton_armijo` avec les ajouts suivants:\n",
    "- Changer les paramètre d'entrées de la fonction pour un `nlp`\n",
    "- Avant d'appeler la recherche linéaire, si `slope = dot(dk, gk)` est plus grand que `-1.0e-4 * norm(dk) * gnorm`, on modifie le système. On fait maximum 5 mise à jour de `λ`, sinon on prend l'opposé du gradient.\n",
    "```\n",
    "    λ = 0.0\n",
    "    while slope ≥ -1.0e-4 * norm(dk) * gnorm\n",
    "      λ = max(1.0e-3, 10 * λ)\n",
    "      dk = - ((Hk + λ * I ) \\ gk)\n",
    "      slope = dot(dk, gk)\n",
    "    end\n",
    "```\n",
    "Ajouter un compteur sur le nombre de mises à jour de `λ` et ajuster `dk = - gk` si la limite est atteinte.\n",
    "- On veut aussi détecter et éventuellement arrêter la boucle `while` si la fonction objectif `fk` devient trop petite/négative (inférieure à `-1e15`), i.e. le problème est non-bornée inférieurement.\n",
    "- On veut ajouter deux critères d'arrêts supplémentaires: \n",
    "  - un compteur sur le nombre d'évaluations de f (maximum 1000). Utiliser `neval_obj(nlp)`.\n",
    "  -  une limite de temps d'execution, `max_time = 60.0`. Utiliser la fonction `time()`.\n",
    "- Enfin, on voudrait aussi voir un message à l'écran si l'algorithme n'a pas trouvé la solution, i.e. il s'est arrêté à cause de la limite sur le nombre d'itérations, temps, évaluation de fonctions, problème non-borné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION: fonction à modifier\n",
    "function newton_armijo(nlp, x0; verbose::Bool = true)\n",
    "  # TODO...\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "x0 = zeros(2)\n",
    "nlp = ADNLPModel(f, x0)\n",
    "\n",
    "newton_armijo(nlp, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: LDLt-Newton avec recherche linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant modifier la méthode de Newton vu précédemment pour utiliser un package qui s'occupe de calculer une factorisation de la matrice hessienne tel que:\n",
    "$$\n",
    "\\nabla^2 f(x) = LDL^T.\n",
    "$$\n",
    "Ce type de factorisation n'est possible que si la matrice hessienne est définie positive, dans le cas contraire on a besoin de régularisé le système comme dans l'exercice précédent.\n",
    "\n",
    "Pour résoudre le système linéaire en utilisant cette factorisation, on va utiliser le package [`LDLFactorizations`](https://github.com/JuliaSmoothOptimizers/LDLFactorizations.jl):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LDLFactorizations, LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tutoriel sur l'utilisation de `LDLFactorizations` est disponible sur la documentation du package sur github ou encore [à ce lien](https://juliasmoothoptimizers.github.io/LDLFactorizations.jl/dev/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple d'utilisation de ce package. La matrice dont on veut calculer la factorisation doit être de type `Symmetric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ones(2,2) #cette matrice symétrique, mais pas du type Symmetric\n",
    "              #à noter que cette matrice n'est pas définie positive.\n",
    "typeof(A) <: Symmetric #false\n",
    "A = Symmetric(A)\n",
    "typeof(A) <: Symmetric #true :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxième étape, le package fait une phase d'analyse de la matrice avec `ldl_analyze` en créant une structure pratique pour les diverses fonctions du package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = -rand(2, 2)\n",
    "sol = rand(2)\n",
    "b = A*sol #on veut résoudre le système A*x=b\n",
    "\n",
    "# LDLFactorizations va en réalité demander la matrice triangulaire supérieure\n",
    "A = Symmetric(triu(A), :U)\n",
    "S = ldl_analyze(A)\n",
    "ldl_factorize!(A, S)\n",
    "x = S \\ b # x = A \\b ça va être résolu par Julia \n",
    "norm(A * x - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0. 1.; 1. 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Symmetric(triu(A), :U)\n",
    "S = ldl_analyze(A)\n",
    "ldl_factorize!(A, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice `A` factorisée par $LDL^T$ n'était pas forcément définie positive. On peut le voir sur les valeurs de $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.d #c'est le vecteur qui correspond à la matrice diagonale D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'optimisation, dans le cas où des valeurs de $D$ sont négatives, i.e. `minimum(S.d) <= 0.`, on ajoutera une correction pour être sûr d'obtenir une direction de descente. On pourra choisir un des deux:\n",
    "- `S.d   = abs.(S.d)`\n",
    "- `S.d .+= -minimum(S.d) + 1e-6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser cette technique pour calculer la direction de descente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: modifier le calcul de la direction avec LDLFactorizations\n",
    "function newton_ldlt_armijo(nlp, x0; verbose::Bool = true)\n",
    "  xk  = x0\n",
    "  fk  = obj(nlp, xk)\n",
    "  gk = grad(nlp, xk)\n",
    "  gnorm = gnorm0 = norm(gk)\n",
    "  k = 0\n",
    "  verbose && @printf \"%2s %9s %9s\\n\" \"k\" \"fk\" \"||∇f(x)||\"\n",
    "  verbose && @printf \"%2d %9.2e %9.1e\\n\" k fk gnorm\n",
    "  while gnorm > 1.0e-6 + 1.0e-6 * gnorm0 && k < 100 && fk > -1e15\n",
    "    Hk = Symmetric(triu(hess(nlp, xk)), :U)\n",
    "    # ... TODO ...\n",
    "    # Sk = ...\n",
    "    dk = - Sk \\ gk\n",
    "    slope = dot(dk, gk)\n",
    "    t = armijo(xk, dk, fk, gk, x -> obj(nlp, x))\n",
    "    xk += t * dk\n",
    "    fk = obj(nlp, xk)\n",
    "    gk = grad(nlp, xk)\n",
    "    gnorm = norm(gk)\n",
    "    k += 1\n",
    "    verbose && @printf \"%2d %9.2e %9.1e %7.1e \\n\" k fk gnorm t\n",
    "  end\n",
    "  return xk\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "nlp = ADNLPModel(f, zeros(2))\n",
    "\n",
    "newton_ldlt_armijo(nlp, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Méthode quasi-Newton: BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode quasi-Newton: BFGS\n",
    "Pour des problèmes de très grandes tailles, il est parfois très coûteux d'évaluer la hessienne du problème d'optimisation (et même le produit hessienne-vecteur). La famille des méthode *quasi-Newton* construit une approximation $B_k$ symétrique de la matrice Hessienne en utilisant seulement le gradient et en mesurant sa variation, et permet quand même d'améliorer significativement les performances comparé à la méthode du gradient.\n",
    "$$\n",
    "s_k = x_{k+1} - x_k, \\quad y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k).\n",
    "$$\n",
    "Par ailleurs la matrice $B_k$ est aussi construite de façon à ce que l'inverse soit connue, il n'y a donc pas de système linéaire à résoudre.\n",
    "\n",
    "La méthode la plus connue dans la famille des méthodes quasi-Newton, est la méthode BFGS (Broyden - Fletcher, Goldfarb, and Shanno) où $B_k$ est définir positive ($B_0 = \\lambda I, \\;\\lambda > 0$).\n",
    "La formule suivante calcule l'inverse de $B_k$ que l'on note $H_k$:\n",
    "$$\n",
    "H_{k+1} = (I - \\rho_k s_ky_k^T)H_k(I-\\rho_ky_ks_k^T) + \\rho_ks_ks_k^T, \\quad \\rho_k = \\frac{1}{y_k^Ts_k}.\n",
    "$$\n",
    "L'algorithme est presque le même que la méthode de Newton à la différence qu'il n'y a pas de système linéaire à résoudre et la direction $d_k$ est à coup sûr une direction de descente. Ainsi la direction de descente est calculée comme suit:\n",
    "$$\n",
    "d_k = - H_k \\nabla f(x_k).\n",
    "$$\n",
    "\n",
    "Comment choisir la matrice $H_0$? On peut éventuellement choisir $I$. Une alternative est d'utiliser $H_0=I$ pour la première itération et ensuite mettre $H_0$ à jour avant de calculer $H_1$ en utilisant:\n",
    "$$\n",
    "H_0 = \\frac{y_k^Ts_k}{y_k^Ty_k}I.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: pour s'assurer que la matrice $H_k$ reste définie positive à toutes les itérations, il faut s'assurer que $y_k^Ts_k>0$. C'est toujours vrai pour des fonctions convexes, mais pas nécessairement dans le cas général. On pourra tester ici la version \"skip\" qui ne mets pas à jour quand cette condition n'est pas vérifiée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: copier-coller votre newton_armijo ici et modifier le calcul de la direction avec la méthode de BFGS inverse skip.\n",
    "function bfgs_quasi_newton_armijo(nlp, x0; verbose::Bool = true)\n",
    "  xk = x0\n",
    "  #à compléter\n",
    "  return xk\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "nlp = ADNLPModel(f, zeros(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4: application à un problème de grande taille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ajouter le package `OptimizationProblems` qui contient, comme son nom l'indique, une collection de problème d'optimisation disponible au format de `JuMP` (dans le sous-module `OptimizationProblems.PureJuMP`) et de `ADNLPModel` (dans le sous-module `OptimizationProblems.ADNLPProblems`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ADNLPModels, OptimizationProblems.ADNLPProblems # Attention si vous ne faites pas using ADNLPModels avant ça ne fonctionne pas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "model = genrose(n=n)\n",
    "@test typeof(model) <: ADNLPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous le souhaitez, il est possible d'accéder à certaines informations sur le problème en accédant à son meta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OptimizationProblems\n",
    "OptimizationProblems.genrose_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est aussi possible d'accéder au meta de tous les problèmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OptimizationProblems.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résoudre le problème `genrose` et un autre problème de la collection en utilisant vos algorithmes précédents.\n",
    "Avant d'utiliser l'algorithme on testera que le problème est bien sans contrainte avec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstrained(nlp) #qui retourne vrai si `nlp` est un problème sans contraintes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use previous functions to solve genrose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
