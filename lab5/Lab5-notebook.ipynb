{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Laboratoire 5: Optimisation avec contraintes et calcul variationnel\n",
    "Tangi Migot et Paul Raynaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Krylov, LinearAlgebra, Logging, NLPModels, NLPModelsIpopt, Printf, SolverCore, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PDENLPModels, Gridap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques commentaires en Julia\n",
    "\n",
    "### Les kwargs: choix optionnels\n",
    "\n",
    "Dans le projet du dernier labo, une des questions demandait d'ajouter une option pour utiliser la fonction `lsmr` ou `lsqr`. C'est le cas typique d'arguments optionnels:\n",
    "- On veut proposer un choix par défaut à l'utilisateur, par exemple `lsqr`;\n",
    "- On veut laisser la possibilité à l'utilisateur de changer;\n",
    "- On voudrait aussi pouvoir ajouter d'autres par la suite (sans avoir à tout modifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function dsol(A, b, ϵ; solver :: Function = lsqr)\n",
    "    (d, stats) = solver(A, b, atol = ϵ)\n",
    "    return d\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noter que l'on donne des valeurs par défaut aux arguments qui apparaissent après le `;`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: Pénalité quadratique pour les ADNLPModels\n",
    "\n",
    "Dans cet exercice, on va étudier une version simple d'une méthode de pénalité quadratique pour les problèmes d'optimisation avec contraintes d'égalité.\n",
    "```math\n",
    "min f(x) s.à c(x) = 0.\n",
    "```\n",
    "Dans les labos précédents, on a déjà utilisé un NLPModel particulier, le ADNLPModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ADNLPModels, LinearAlgebra, Test\n",
    "fH(x) = (x[2]+x[1].^2-11)^2 + (x[1]+x[2].^2-7)^2\n",
    "x0H = [10., 20.]\n",
    "cH(x) = [x[1]-1]\n",
    "himmelblau = ADNLPModel(fH, x0H, cH, [0.], [0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: dans toute la suite de l'exercice on suppose que les bornes sur les contraintes `nlp.meta.lcon` et `nlp.meta.ucon` sont 0 pour simplifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Transformer un ADNLPModel en un problème pénalisé\n",
    "Coder la fonction `quad_penalty_adnlp` qui prend en entrée un ADNLPModel, et un paramètre ρ et qui retourne un nouveau ADNLPModel qui correspond au problème sans contrainte:\n",
    "$$\n",
    "\\min_x f(x) + \\frac{\\rho}{2}\\|c(x)\\|^2.\n",
    "$$\n",
    "Remarque: on peut accèder aux fonctions f et c par `NLPModels.obj()` et `NLPModels.cons()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function quad_penalty_adnlp(nlp :: ADNLPModel, ρ :: Real)\n",
    "    # TODO\n",
    "   return nlp_quad\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faire des tests pour vérifier que ça fonctionne.\n",
    "fH(x) = (x[2]+x[1].^2-11).^2+(x[1]+x[2].^2-7).^2\n",
    "x0H = [10., 20.]\n",
    "himmelblau = ADNLPModel(fH, x0H)\n",
    "\n",
    "himmelblau_quad = quad_penalty_adnlp(himmelblau, 1)\n",
    "@test himmelblau_quad.meta.ncon == 0\n",
    "@test obj(himmelblau_quad, zeros(2)) == 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajouter au moins un autre test similaire avec des contraintes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter un test au cas ou `nlp.meta.lcon` ou `nlp.meta.ucon` ont des composantes differentes de 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: KKT\n",
    "Coder une fonction `KKT_eq_constraint(nlp :: AbstractNLPModel, x, λ)` qui vérifie si le point `x` avec multiplicateur de Lagrange `λ` satisfait les conditions KKT d'un problème avec contraintes d'égalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function KKT_eq_constraint(nlp :: AbstractNLPModel, x, λ)\n",
    "   # TODO\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: méthode de pénalité quadratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLPModelsIpopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function quad_penalty(nlp      :: AbstractNLPModel,\n",
    "                      x        :: AbstractVector; \n",
    "                      ϵ        :: AbstractFloat = 1e-3,\n",
    "                      η        :: AbstractFloat = 1e6, \n",
    "                      σ        :: AbstractFloat = 2.0,\n",
    "                      max_eval :: Int = 1_000, \n",
    "                      max_time :: AbstractFloat = 60.,\n",
    "                      max_iter :: Int = typemax(Int64)\n",
    "                      )\n",
    "    ##### Initialiser cx et gx au point x;\n",
    "    cx = # Initialiser la violation des contraintes\n",
    "    gx = # Initialiser le gradient\n",
    "    ######################################################\n",
    "    normcx = normcx_old = norm(cx)\n",
    "\n",
    "    ρ = 1.\n",
    "\n",
    "    iter = 0    \n",
    "\n",
    "    el_time = 0.0\n",
    "    tired   = neval_cons(nlp) > max_eval || el_time > max_time\n",
    "    status  = :unknown\n",
    "\n",
    "    start_time = time()\n",
    "    too_small  = false\n",
    "    normdual   = norm(gx) #exceptionnellement on ne va pas vérifier toute l'optimalité au début.\n",
    "    optimal    = max(normcx, normdual) ≤ ϵ\n",
    "    \n",
    "    nlp_quad   = quad_penalty_adnlp(nlp, ρ)\n",
    "\n",
    "    @info log_header([:iter, :nf, :primal, :status, :nd, :Δ],\n",
    "    [Int, Int, Float64, String, Float64, Float64],\n",
    "    hdr_override=Dict(:nf => \"#F\", :primal => \"‖F(x)‖\", :nd => \"‖d‖\"))\n",
    "\n",
    "    while !(optimal || tired || too_small)\n",
    "\n",
    "        #Appeler Ipopt pour résoudre le problème pénalisé en partant du point x0 = x.\n",
    "        #utiliser l'option print_level = 0 pour enlever les affichages d'ipopt.\n",
    "        stats = #...\n",
    "        ################################################\n",
    "      \n",
    "        if stats.status == :first_order\n",
    "            ###### Mettre à jour cx avec la solution renvoyé par Ipopt\n",
    "            x = #...\n",
    "            cx = #...\n",
    "            ##########################################################\n",
    "            normcx_old = normcx\n",
    "            normcx = norm(cx)\n",
    "        end\n",
    "        \n",
    "        if normcx_old > 0.95 * normcx\n",
    "            ρ *= σ\n",
    "        end\n",
    "\n",
    "        @info log_row(Any[iter, neval_cons(nlp), normcx, stats.status])\n",
    "        \n",
    "        nlp_quad   = quad_penalty_adnlp(nlp, ρ)\n",
    "\n",
    "        el_time      = time() - start_time\n",
    "        iter   += 1\n",
    "        many_evals   = neval_cons(nlp) > max_eval\n",
    "        iter_limit   = iter > max_iter\n",
    "        tired        = many_evals || el_time > max_time || iter_limit || ρ ≥ η\n",
    "        ##### Utiliser la réalisabilité dual renvoyé par Ipopt pour `normdual`\n",
    "        normdual     = #....\n",
    "        ###################################################################\n",
    "        optimal      = max(normcx, normdual) ≤ ϵ\n",
    "    end\n",
    "\n",
    "    status = if optimal \n",
    "        :first_order\n",
    "    elseif tired\n",
    "        if neval_cons(nlp) > max_eval\n",
    "            :max_eval\n",
    "        elseif el_time > max_time\n",
    "            :max_time\n",
    "        elseif iter > max_iter\n",
    "            :max_iter\n",
    "        else\n",
    "            :unknown_tired\n",
    "        end\n",
    "    elseif too_small\n",
    "        :stalled\n",
    "    else\n",
    "        :unknown\n",
    "    end\n",
    "\n",
    "    return GenericExecutionStats(nlp, status = status, solution = x,\n",
    "                                 objective = obj(nlp, x),\n",
    "                                 primal_feas = normcx,\n",
    "                                 dual_feas = normdual,\n",
    "                                 iter = iter, \n",
    "                                 elapsed_time = el_time,\n",
    "                                 solver_specific = Dict(:penalty => ρ))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faire des tests pour vérifier que ça fonctionne.\n",
    "stats = quad_penalty(himmelblau, x0H)\n",
    "@test stats.status == :first_order\n",
    "@test stats.solution ≈ [1.0008083416169895, 2.709969135758311] atol=1e-2\n",
    "@test norm(cons(himmelblau, stats.solution)) ≈ 0. atol=1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier que la solution rendue vérifie les conditions KKT avec la fonction de la question précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fichier de tests à demander.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: Calcul Variationnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, on considère le problème de calcul variationnel suivant:\n",
    "$$\n",
    "\\min \\int_0^1 (\\dot{x}(t)^2+2x(t)^2)e^t dt, \\quad x(0)=0, x(1)=e - e^{-2}\n",
    "$$\n",
    "\n",
    "modélisé avec `PDENLPModels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cv_model(n :: Int)\n",
    "\n",
    "  domain = (0,1) # set the domain\n",
    "  partition = n\n",
    "  model = CartesianDiscreteModel(domain,partition) # set discretization\n",
    "    \n",
    "  labels = get_face_labeling(model)\n",
    "  add_tag_from_tags!(labels,\"diri1\",[2])\n",
    "  add_tag_from_tags!(labels,\"diri0\",[1]) # boundary conditions\n",
    "\n",
    "  order=1\n",
    "  valuetype=Float64\n",
    "  reffe = ReferenceFE(lagrangian, valuetype, order)\n",
    "  V0 = TestFESpace(model, reffe; conformity=:H1, dirichlet_tags=[\"diri0\",\"diri1\"])\n",
    "  U = TrialFESpace(V0,[0., exp(1)-exp(-2)])\n",
    "\n",
    "  trian = Triangulation(model)\n",
    "  degree = 2\n",
    "  dΩ = Measure(trian,degree) # integration machinery\n",
    "\n",
    "  # Our objective function\n",
    "  w(x) = exp(x[1])\n",
    "  function f(y)\n",
    "    ∫((∇(y)⊙∇(y) + 2 * y * y) * w) * dΩ\n",
    "  end\n",
    "\n",
    "  xin = zeros(Gridap.FESpaces.num_free_dofs(U))\n",
    "  nlp = GridapPDENLPModel(xin, f, trian, U, V0)\n",
    "  return nlp\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Résoudre\n",
    "Résoudre le NLPModel généré par la fonction `cv_model` pour `n = 16` avec `ipopt` et afficher la solution (attention la solution rendue ne contient pas les valeurs aux bords qu'il faut rajouter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Convergence en `n`\n",
    "Afficher sur un même graphique la solution obtenue par `ipopt` pour plusieurs valeurs de `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Comparer à la solution exacte\n",
    "\n",
    "La solution exacte est $x(t)=e^t - e^{-2t}$ et la valeur optimale est $e^3 - 2e^{-3}+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
